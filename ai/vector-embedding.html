<!DOCTYPE html>
<html lang="zh-CN" dir="ltr">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>大模型中的向量嵌入（Vector Embedding） | Yabin's Tech Journey</title>
    <meta name="description" content="算法、java、AI">
    <meta name="generator" content="VitePress v1.6.3">
    <link rel="preload stylesheet" href="/assets/style.T87138lQ.css" as="style">
    <link rel="preload stylesheet" href="/vp-icons.css" as="style">
    
    <script type="module" src="/assets/app.CrWhcqTx.js"></script>
    <link rel="preload" href="/assets/inter-roman-latin.Di8DUHzh.woff2" as="font" type="font/woff2" crossorigin="">
    <link rel="modulepreload" href="/assets/chunks/theme.D6KX6WHB.js">
    <link rel="modulepreload" href="/assets/chunks/framework.DK1-H3E1.js">
    <link rel="modulepreload" href="/assets/ai_vector-embedding.md.DM0LQQU-.lean.js">
    <meta name="keywords" content="技术博客,算法,系统设计,数据库,LeetCode">
    <meta name="author" content="Wang Yabin">
    <meta property="og:title" content="Yabin's Tech Journey">
    <meta property="og:description" content="技术探索者的博客 - 分享算法、系统设计与工程实践">
    <script src="/password-protect.js"></script>
    <script>(function(){document.documentElement.classList.add("dark"),localStorage.setItem("vitepress-theme-appearance","dark")})();</script>
    <script id="check-mac-os">document.documentElement.classList.toggle("mac",/Mac|iPhone|iPod|iPad/i.test(navigator.platform));</script>
  </head>
  <body>
    <div id="app"><div class="Layout" data-v-5d98c3a5><!--[--><!--]--><!--[--><span tabindex="-1" data-v-0b0ada53></span><a href="#VPContent" class="VPSkipLink visually-hidden" data-v-0b0ada53>Skip to content</a><!--]--><!----><header class="VPNav" data-v-5d98c3a5 data-v-ae24b3ad><div class="VPNavBar" data-v-ae24b3ad data-v-6aa21345><div class="wrapper" data-v-6aa21345><div class="container" data-v-6aa21345><div class="title" data-v-6aa21345><div class="VPNavBarTitle has-sidebar" data-v-6aa21345 data-v-1168a8e4><a class="title" href="/" data-v-1168a8e4><!--[--><!--]--><!----><span data-v-1168a8e4>Yabin's Tech Journey</span><!--[--><!--]--></a></div></div><div class="content" data-v-6aa21345><div class="content-body" data-v-6aa21345><!--[--><!--]--><div class="VPNavBarSearch search" data-v-6aa21345><!--[--><!----><div id="local-search"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><span class="vp-icon DocSearch-Search-Icon"></span><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"><kbd class="DocSearch-Button-Key"></kbd><kbd class="DocSearch-Button-Key">K</kbd></span></button></div><!--]--></div><nav aria-labelledby="main-nav-aria-label" class="VPNavBarMenu menu" data-v-6aa21345 data-v-dc692963><span id="main-nav-aria-label" class="visually-hidden" data-v-dc692963> Main Navigation </span><!--[--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/30-day-algorithm/" tabindex="0" data-v-dc692963 data-v-e56f3d57><!--[--><span data-v-e56f3d57>🚀 30天刷题计划</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/interview/" tabindex="0" data-v-dc692963 data-v-e56f3d57><!--[--><span data-v-e56f3d57>☕ Java 面试</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/ai/" tabindex="0" data-v-dc692963 data-v-e56f3d57><!--[--><span data-v-e56f3d57>🤖 AI 探索</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/others/" tabindex="0" data-v-dc692963 data-v-e56f3d57><!--[--><span data-v-e56f3d57>📝 其他文章</span><!--]--></a><!--]--><!--]--></nav><!----><!----><div class="VPSocialLinks VPNavBarSocialLinks social-links" data-v-6aa21345 data-v-0394ad82 data-v-7bc22406><!--[--><a class="VPSocialLink no-icon" href="https://github.com/yabwang" aria-label="github" target="_blank" rel="noopener" data-v-7bc22406 data-v-bd121fe5><span class="vpi-social-github"></span></a><!--]--></div><div class="VPFlyout VPNavBarExtra extra" data-v-6aa21345 data-v-bb2aa2f0 data-v-cf11d7a2><button type="button" class="button" aria-haspopup="true" aria-expanded="false" aria-label="extra navigation" data-v-cf11d7a2><span class="vpi-more-horizontal icon" data-v-cf11d7a2></span></button><div class="menu" data-v-cf11d7a2><div class="VPMenu" data-v-cf11d7a2 data-v-b98bc113><!----><!--[--><!--[--><!----><!----><div class="group" data-v-bb2aa2f0><div class="item social-links" data-v-bb2aa2f0><div class="VPSocialLinks social-links-list" data-v-bb2aa2f0 data-v-7bc22406><!--[--><a class="VPSocialLink no-icon" href="https://github.com/yabwang" aria-label="github" target="_blank" rel="noopener" data-v-7bc22406 data-v-bd121fe5><span class="vpi-social-github"></span></a><!--]--></div></div></div><!--]--><!--]--></div></div></div><!--[--><!--]--><button type="button" class="VPNavBarHamburger hamburger" aria-label="mobile navigation" aria-expanded="false" aria-controls="VPNavScreen" data-v-6aa21345 data-v-e5dd9c1c><span class="container" data-v-e5dd9c1c><span class="top" data-v-e5dd9c1c></span><span class="middle" data-v-e5dd9c1c></span><span class="bottom" data-v-e5dd9c1c></span></span></button></div></div></div></div><div class="divider" data-v-6aa21345><div class="divider-line" data-v-6aa21345></div></div></div><!----></header><div class="VPLocalNav has-sidebar empty" data-v-5d98c3a5 data-v-a6f0e41e><div class="container" data-v-a6f0e41e><button class="menu" aria-expanded="false" aria-controls="VPSidebarNav" data-v-a6f0e41e><span class="vpi-align-left menu-icon" data-v-a6f0e41e></span><span class="menu-text" data-v-a6f0e41e>Menu</span></button><div class="VPLocalNavOutlineDropdown" style="--vp-vh:0px;" data-v-a6f0e41e data-v-8a42e2b4><button data-v-8a42e2b4>Return to top</button><!----></div></div></div><aside class="VPSidebar" data-v-5d98c3a5 data-v-319d5ca6><div class="curtain" data-v-319d5ca6></div><nav class="nav" id="VPSidebarNav" aria-labelledby="sidebar-aria-label" tabindex="-1" data-v-319d5ca6><span class="visually-hidden" id="sidebar-aria-label" data-v-319d5ca6> Sidebar Navigation </span><!--[--><!--]--><!--[--><div class="no-transition group" data-v-c40bc020><section class="VPSidebarItem level-0 has-active" data-v-c40bc020 data-v-b3fd67f8><!----><div class="items" data-v-b3fd67f8><!--[--><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/ai/latest-developments.html" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>AI 最新进展</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/ai/vector-embedding.html" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>大模型中的向量嵌入（Vector Embedding）</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/ai/prompt-engineering.html" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>Prompt 工程：大语言模型交互的艺术</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/ai/living-files-theory.html" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>活文件理论 (The "Living Files" Theory)</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/ai/openclaw-memory-architecture.html" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>🧠 OpenClaw 记忆系统架构</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><!--]--><!--[--><!--]--></nav></aside><div class="VPContent has-sidebar" id="VPContent" data-v-5d98c3a5 data-v-1428d186><div class="VPDoc has-sidebar has-aside" data-v-1428d186 data-v-39a288b8><!--[--><!--]--><div class="container" data-v-39a288b8><div class="aside" data-v-39a288b8><div class="aside-curtain" data-v-39a288b8></div><div class="aside-container" data-v-39a288b8><div class="aside-content" data-v-39a288b8><div class="VPDocAside" data-v-39a288b8 data-v-3f215769><!--[--><!--]--><!--[--><!--]--><nav aria-labelledby="doc-outline-aria-label" class="VPDocAsideOutline" data-v-3f215769 data-v-a5bbad30><div class="content" data-v-a5bbad30><div class="outline-marker" data-v-a5bbad30></div><div aria-level="2" class="outline-title" id="doc-outline-aria-label" role="heading" data-v-a5bbad30>目录</div><ul class="VPDocOutlineItem root" data-v-a5bbad30 data-v-b933a997><!--[--><!--]--></ul></div></nav><!--[--><!--]--><div class="spacer" data-v-3f215769></div><!--[--><!--]--><!----><!--[--><!--]--><!--[--><!--]--></div></div></div></div><div class="content" data-v-39a288b8><div class="content-container" data-v-39a288b8><!--[--><!--]--><main class="main" data-v-39a288b8><div style="position:relative;" class="vp-doc _ai_vector-embedding" data-v-39a288b8><div><h1 id="大模型中的向量嵌入-vector-embedding" tabindex="-1">大模型中的向量嵌入（Vector Embedding） <a class="header-anchor" href="#大模型中的向量嵌入-vector-embedding" aria-label="Permalink to &quot;大模型中的向量嵌入（Vector Embedding）&quot;">​</a></h1><blockquote><p>📅 最后更新：2025年1月<br> 🎯 主题：向量嵌入原理、应用与实践</p></blockquote><h2 id="引言" tabindex="-1">引言 <a class="header-anchor" href="#引言" aria-label="Permalink to &quot;引言&quot;">​</a></h2><p>向量嵌入（Vector Embedding）是大语言模型中的核心技术之一，它将文本、图像等离散数据转换为连续的向量表示，使得计算机能够理解和处理语义信息。在 RAG（检索增强生成）、推荐系统、语义搜索等应用中，向量嵌入发挥着至关重要的作用。</p><h2 id="什么是向量嵌入" tabindex="-1">什么是向量嵌入 <a class="header-anchor" href="#什么是向量嵌入" aria-label="Permalink to &quot;什么是向量嵌入&quot;">​</a></h2><h3 id="基本定义" tabindex="-1">基本定义 <a class="header-anchor" href="#基本定义" aria-label="Permalink to &quot;基本定义&quot;">​</a></h3><p><strong>向量嵌入（Embedding）</strong> 是一种将高维且通常离散的输入数据（如单词、短语、句子、文档等）映射到低维连续向量空间中的技术。简单来说，它是将文本转换成能表达语义信息的浮点数向量。</p><h3 id="为什么需要向量嵌入" tabindex="-1">为什么需要向量嵌入 <a class="header-anchor" href="#为什么需要向量嵌入" aria-label="Permalink to &quot;为什么需要向量嵌入&quot;">​</a></h3><ol><li><strong>语义表示</strong>：将文本转换为数值向量，使计算机能够理解和比较文本的语义</li><li><strong>相似度计算</strong>：通过向量间的数学距离反映文本间的语义相关性</li><li><strong>降维处理</strong>：将高维稀疏的文本数据转换为低维稠密的向量表示</li><li><strong>计算效率</strong>：向量运算比文本匹配更高效，支持大规模数据处理</li></ol><h2 id="核心原理与工作机制" tabindex="-1">核心原理与工作机制 <a class="header-anchor" href="#核心原理与工作机制" aria-label="Permalink to &quot;核心原理与工作机制&quot;">​</a></h2><h3 id="从文本到向量的转换过程" tabindex="-1">从文本到向量的转换过程 <a class="header-anchor" href="#从文本到向量的转换过程" aria-label="Permalink to &quot;从文本到向量的转换过程&quot;">​</a></h3><p>向量嵌入的生成过程通常包括以下步骤：</p><ol><li><strong>文本预处理</strong>：对输入文本进行清洗、分词等预处理</li><li><strong>Token 化</strong>：将文本拆分成 token（词或子词单元）</li><li><strong>编号映射</strong>：将 token 映射成数字编号（Token ID）</li><li><strong>Embedding 查询</strong>：根据编号从 Embedding 矩阵中查询对应的向量</li><li><strong>向量聚合</strong>：对于多个 token，通过平均、池化等方式生成最终向量</li></ol><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>文本 → Token → 编号 → Embedding矩阵 → 向量</span></span>
<span class="line"><span>&quot;Hello World&quot; → [&quot;Hello&quot;, &quot;World&quot;] → [101, 102] → [[0.1, 0.2, ...], [0.3, 0.4, ...]] → [0.2, 0.3, ...]</span></span></code></pre></div><h3 id="语义相似性原理" tabindex="-1">语义相似性原理 <a class="header-anchor" href="#语义相似性原理" aria-label="Permalink to &quot;语义相似性原理&quot;">​</a></h3><p>在 Embedding 空间中，语义相似的数据点具有相近的向量表示：</p><ul><li><strong>相似文本</strong>：如&quot;猫&quot;和&quot;狗&quot;的向量距离很近</li><li><strong>不相关文本</strong>：如&quot;房子&quot;和&quot;你好&quot;的向量距离很远</li><li><strong>语义关系</strong>：通过向量运算可以捕捉同义词、反义词、上下位关系等</li></ul><h3 id="关键技术" tabindex="-1">关键技术 <a class="header-anchor" href="#关键技术" aria-label="Permalink to &quot;关键技术&quot;">​</a></h3><p>现代向量嵌入技术主要基于：</p><ul><li><strong>Transformer 架构</strong>：采用自注意力机制捕捉序列中的长距离依赖关系</li><li><strong>预训练模型</strong>：在大规模语料上预训练，学习通用的语义表示</li><li><strong>并行计算</strong>：通过 GPU 加速，支持大规模向量计算</li><li><strong>内存优化</strong>：采用量化、压缩等技术降低存储和计算成本</li></ul><h2 id="技术实现" tabindex="-1">技术实现 <a class="header-anchor" href="#技术实现" aria-label="Permalink to &quot;技术实现&quot;">​</a></h2><h3 id="基于-transformer-的-embedding-模型" tabindex="-1">基于 Transformer 的 Embedding 模型 <a class="header-anchor" href="#基于-transformer-的-embedding-模型" aria-label="Permalink to &quot;基于 Transformer 的 Embedding 模型&quot;">​</a></h3><p>主流的 Embedding 模型通常基于 Transformer 架构：</p><h4 id="_1-bert-系列" tabindex="-1">1. BERT 系列 <a class="header-anchor" href="#_1-bert-系列" aria-label="Permalink to &quot;1. BERT 系列&quot;">​</a></h4><ul><li><strong>BERT</strong>：双向编码器，适合理解任务</li><li><strong>RoBERTa</strong>：优化训练策略的 BERT</li><li><strong>ALBERT</strong>：参数共享，降低模型大小</li></ul><h4 id="_2-gpt-系列" tabindex="-1">2. GPT 系列 <a class="header-anchor" href="#_2-gpt-系列" aria-label="Permalink to &quot;2. GPT 系列&quot;">​</a></h4><ul><li><strong>GPT</strong>：自回归模型，适合生成任务</li><li><strong>GPT-3/4</strong>：大规模预训练模型</li></ul><h4 id="_3-专门优化的-embedding-模型" tabindex="-1">3. 专门优化的 Embedding 模型 <a class="header-anchor" href="#_3-专门优化的-embedding-模型" aria-label="Permalink to &quot;3. 专门优化的 Embedding 模型&quot;">​</a></h4><ul><li><strong>Sentence-BERT (SBERT)</strong>：专门为句子嵌入优化</li><li><strong>OpenAI Embeddings</strong>：text-embedding-ada-002 等</li><li><strong>BGE (BAAI General Embedding)</strong>：中文优化的嵌入模型</li></ul><h3 id="训练方法" tabindex="-1">训练方法 <a class="header-anchor" href="#训练方法" aria-label="Permalink to &quot;训练方法&quot;">​</a></h3><h4 id="_1-预训练-pre-training" tabindex="-1">1. 预训练（Pre-training） <a class="header-anchor" href="#_1-预训练-pre-training" aria-label="Permalink to &quot;1. 预训练（Pre-training）&quot;">​</a></h4><ul><li>在大规模无标注语料上训练</li><li>学习通用的语言表示能力</li><li>常见任务：掩码语言模型（MLM）、下一句预测（NSP）</li></ul><h4 id="_2-微调-fine-tuning" tabindex="-1">2. 微调（Fine-tuning） <a class="header-anchor" href="#_2-微调-fine-tuning" aria-label="Permalink to &quot;2. 微调（Fine-tuning）&quot;">​</a></h4><ul><li>在特定任务数据上微调</li><li>提升特定领域的表现</li><li>支持监督学习和对比学习</li></ul><h4 id="_3-对比学习-contrastive-learning" tabindex="-1">3. 对比学习（Contrastive Learning） <a class="header-anchor" href="#_3-对比学习-contrastive-learning" aria-label="Permalink to &quot;3. 对比学习（Contrastive Learning）&quot;">​</a></h4><ul><li>通过正负样本对比学习</li><li>拉近相似文本，推远不相似文本</li><li>提升嵌入质量</li></ul><h2 id="主要应用场景" tabindex="-1">主要应用场景 <a class="header-anchor" href="#主要应用场景" aria-label="Permalink to &quot;主要应用场景&quot;">​</a></h2><h3 id="_1-rag-检索增强生成" tabindex="-1">1. RAG（检索增强生成） <a class="header-anchor" href="#_1-rag-检索增强生成" aria-label="Permalink to &quot;1. RAG（检索增强生成）&quot;">​</a></h3><p><strong>应用流程</strong>：</p><ol><li>将文档库转换为向量并存储到向量数据库</li><li>将用户查询转换为向量</li><li>通过向量相似度检索相关文档片段</li><li>将检索到的文档作为上下文输入大模型生成答案</li></ol><p><strong>优势</strong>：</p><ul><li>突破模型上下文长度限制</li><li>提供实时、准确的外部知识</li><li>支持知识更新和领域定制</li></ul><h3 id="_2-推荐系统" tabindex="-1">2. 推荐系统 <a class="header-anchor" href="#_2-推荐系统" aria-label="Permalink to &quot;2. 推荐系统&quot;">​</a></h3><p><strong>应用方式</strong>：</p><ul><li>将用户行为、商品属性转换为向量</li><li>通过向量相似度计算用户-商品匹配度</li><li>实现个性化推荐</li></ul><p><strong>效果</strong>：</p><ul><li>提升推荐准确性</li><li>发现潜在兴趣关联</li><li>支持冷启动问题</li></ul><h3 id="_3-语义搜索" tabindex="-1">3. 语义搜索 <a class="header-anchor" href="#_3-语义搜索" aria-label="Permalink to &quot;3. 语义搜索&quot;">​</a></h3><p><strong>特点</strong>：</p><ul><li>理解查询意图，而非简单的关键词匹配</li><li>支持同义词、相关词检索</li><li>提升搜索相关性</li></ul><h3 id="_4-文本相似度与聚类" tabindex="-1">4. 文本相似度与聚类 <a class="header-anchor" href="#_4-文本相似度与聚类" aria-label="Permalink to &quot;4. 文本相似度与聚类&quot;">​</a></h3><p><strong>应用</strong>：</p><ul><li><strong>相似度判断</strong>：判断两段文本的语义相似度</li><li><strong>文本聚类</strong>：将相似文本归为一类</li><li><strong>主题发现</strong>：通过聚类发现文本主题</li><li><strong>异常检测</strong>：识别与正常模式差异较大的文本</li></ul><h3 id="_5-多模态应用" tabindex="-1">5. 多模态应用 <a class="header-anchor" href="#_5-多模态应用" aria-label="Permalink to &quot;5. 多模态应用&quot;">​</a></h3><p><strong>扩展</strong>：</p><ul><li><strong>图像嵌入</strong>：将图像转换为向量</li><li><strong>跨模态检索</strong>：文本-图像、图像-图像检索</li><li><strong>统一表示</strong>：文本、图像、音频统一到同一向量空间</li></ul><h2 id="相似度计算" tabindex="-1">相似度计算 <a class="header-anchor" href="#相似度计算" aria-label="Permalink to &quot;相似度计算&quot;">​</a></h2><p>向量嵌入的相似度计算是应用的核心，常用的方法包括：</p><h3 id="_1-余弦相似度-cosine-similarity" tabindex="-1">1. 余弦相似度（Cosine Similarity） <a class="header-anchor" href="#_1-余弦相似度-cosine-similarity" aria-label="Permalink to &quot;1. 余弦相似度（Cosine Similarity）&quot;">​</a></h3><p>最常用的相似度计算方法，衡量向量方向的相似性：</p><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> cosine_similarity</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(vec1, vec2):</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">    &quot;&quot;&quot;</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">    计算两个向量的余弦相似度</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">    返回值范围：[-1, 1]，1 表示完全相同，-1 表示完全相反</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">    &quot;&quot;&quot;</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    dot_product </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> sum</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(a </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">*</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> b </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> a, b </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> zip</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(vec1, vec2))</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    norm1 </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> sum</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(a </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">*</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> a </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> a </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> vec1) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">**</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 0.5</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    norm2 </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> sum</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(b </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">*</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> b </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> b </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> vec2) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">**</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 0.5</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    </span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    if</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> norm1 </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">==</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 0</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> or</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> norm2 </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">==</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">        return</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 0.0</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    </span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    return</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> dot_product </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">/</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (norm1 </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">*</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> norm2)</span></span></code></pre></div><p><strong>特点</strong>：</p><ul><li>不受向量长度影响，只关注方向</li><li>适合高维稀疏向量</li><li>计算效率高</li></ul><h3 id="_2-欧氏距离-euclidean-distance" tabindex="-1">2. 欧氏距离（Euclidean Distance） <a class="header-anchor" href="#_2-欧氏距离-euclidean-distance" aria-label="Permalink to &quot;2. 欧氏距离（Euclidean Distance）&quot;">​</a></h3><p>衡量向量在空间中的实际距离：</p><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> numpy </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">as</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> np</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> euclidean_distance</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(vec1, vec2):</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">    &quot;&quot;&quot;</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">    计算两个向量的欧氏距离</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">    距离越小，相似度越高</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">    &quot;&quot;&quot;</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    return</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> np.sqrt(np.sum((np.array(vec1) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> np.array(vec2)) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">**</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">))</span></span></code></pre></div><p><strong>特点</strong>：</p><ul><li>直观理解：空间中的直线距离</li><li>受向量长度影响</li><li>适合低维稠密向量</li></ul><h3 id="_3-点积-dot-product" tabindex="-1">3. 点积（Dot Product） <a class="header-anchor" href="#_3-点积-dot-product" aria-label="Permalink to &quot;3. 点积（Dot Product）&quot;">​</a></h3><p>向量内积，简单高效：</p><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> dot_product</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(vec1, vec2):</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">    &quot;&quot;&quot;</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">    计算两个向量的点积</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">    值越大，相似度越高（通常需要归一化）</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">    &quot;&quot;&quot;</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    return</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> sum</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(a </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">*</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> b </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> a, b </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> zip</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(vec1, vec2))</span></span></code></pre></div><p><strong>特点</strong>：</p><ul><li>计算最快</li><li>需要向量归一化才能准确反映相似度</li><li>常用于大规模检索场景</li></ul><h3 id="_4-曼哈顿距离-manhattan-distance" tabindex="-1">4. 曼哈顿距离（Manhattan Distance） <a class="header-anchor" href="#_4-曼哈顿距离-manhattan-distance" aria-label="Permalink to &quot;4. 曼哈顿距离（Manhattan Distance）&quot;">​</a></h3><p>L1 距离，适合某些特定场景：</p><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> manhattan_distance</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(vec1, vec2):</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">    &quot;&quot;&quot;</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">    计算两个向量的曼哈顿距离</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">    &quot;&quot;&quot;</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    return</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> sum</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">abs</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(a </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> b) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> a, b </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> zip</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(vec1, vec2))</span></span></code></pre></div><h2 id="工具与框架" tabindex="-1">工具与框架 <a class="header-anchor" href="#工具与框架" aria-label="Permalink to &quot;工具与框架&quot;">​</a></h2><h3 id="_1-hugging-face-transformers" tabindex="-1">1. Hugging Face Transformers <a class="header-anchor" href="#_1-hugging-face-transformers" aria-label="Permalink to &quot;1. Hugging Face Transformers&quot;">​</a></h3><p><strong>特点</strong>：</p><ul><li>提供丰富的预训练 Embedding 模型</li><li>简单易用的 API</li><li>支持多种模型架构</li></ul><p><strong>示例</strong>：</p><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">from</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> sentence_transformers </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> SentenceTransformer</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">model </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> SentenceTransformer(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;all-MiniLM-L6-v2&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">embeddings </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> model.encode([</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;Hello world&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;How are you?&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">])</span></span></code></pre></div><h3 id="_2-openai-embeddings-api" tabindex="-1">2. OpenAI Embeddings API <a class="header-anchor" href="#_2-openai-embeddings-api" aria-label="Permalink to &quot;2. OpenAI Embeddings API&quot;">​</a></h3><p><strong>特点</strong>：</p><ul><li>云端服务，无需本地部署</li><li>高质量嵌入模型（text-embedding-ada-002, text-embedding-3-small/large）</li><li>支持多语言</li></ul><p><strong>示例</strong>：</p><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> openai</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">response </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> openai.Embedding.create(</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">    input</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;Your text here&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">    model</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;text-embedding-ada-002&quot;</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">embedding </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> response[</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;data&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">][</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">][</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;embedding&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]</span></span></code></pre></div><h3 id="_3-langchain" tabindex="-1">3. LangChain <a class="header-anchor" href="#_3-langchain" aria-label="Permalink to &quot;3. LangChain&quot;">​</a></h3><p><strong>特点</strong>：</p><ul><li>集成多种 Embedding 模型</li><li>提供向量存储和检索功能</li><li>支持 RAG 应用开发</li></ul><h3 id="_4-向量数据库" tabindex="-1">4. 向量数据库 <a class="header-anchor" href="#_4-向量数据库" aria-label="Permalink to &quot;4. 向量数据库&quot;">​</a></h3><p><strong>主流选择</strong>：</p><ul><li><strong>Pinecone</strong>：云端向量数据库，易于使用</li><li><strong>Weaviate</strong>：开源向量搜索引擎</li><li><strong>Milvus</strong>：高性能开源向量数据库</li><li><strong>Chroma</strong>：轻量级嵌入式向量数据库</li><li><strong>Qdrant</strong>：高性能向量搜索引擎</li></ul><h2 id="最佳实践" tabindex="-1">最佳实践 <a class="header-anchor" href="#最佳实践" aria-label="Permalink to &quot;最佳实践&quot;">​</a></h2><h3 id="_1-向量维度选择" tabindex="-1">1. 向量维度选择 <a class="header-anchor" href="#_1-向量维度选择" aria-label="Permalink to &quot;1. 向量维度选择&quot;">​</a></h3><ul><li><strong>低维（128-256）</strong>：计算快，适合大规模检索，但表达能力有限</li><li><strong>中维（384-512）</strong>：平衡性能和效果，推荐用于大多数场景</li><li><strong>高维（768-1536）</strong>：表达能力强，但计算成本高，适合精度要求高的场景</li></ul><h3 id="_2-相似度阈值设置" tabindex="-1">2. 相似度阈值设置 <a class="header-anchor" href="#_2-相似度阈值设置" aria-label="Permalink to &quot;2. 相似度阈值设置&quot;">​</a></h3><ul><li><strong>精确匹配</strong>：阈值 0.9-0.95</li><li><strong>相关匹配</strong>：阈值 0.7-0.85</li><li><strong>模糊匹配</strong>：阈值 0.5-0.7</li></ul><p>根据具体应用场景调整阈值，平衡召回率和准确率。</p><h3 id="_3-批量处理优化" tabindex="-1">3. 批量处理优化 <a class="header-anchor" href="#_3-批量处理优化" aria-label="Permalink to &quot;3. 批量处理优化&quot;">​</a></h3><ul><li>使用批量编码减少 API 调用</li><li>利用 GPU 加速向量计算</li><li>采用异步处理提升吞吐量</li></ul><h3 id="_4-向量归一化" tabindex="-1">4. 向量归一化 <a class="header-anchor" href="#_4-向量归一化" aria-label="Permalink to &quot;4. 向量归一化&quot;">​</a></h3><ul><li>对向量进行 L2 归一化，使余弦相似度等于点积</li><li>提升计算效率和数值稳定性</li></ul><h3 id="_5-混合检索策略" tabindex="-1">5. 混合检索策略 <a class="header-anchor" href="#_5-混合检索策略" aria-label="Permalink to &quot;5. 混合检索策略&quot;">​</a></h3><ul><li><strong>向量检索</strong>：语义相似度</li><li><strong>关键词检索</strong>：精确匹配</li><li><strong>混合检索</strong>：结合两者优势，提升检索质量</li></ul><h3 id="_6-模型选择建议" tabindex="-1">6. 模型选择建议 <a class="header-anchor" href="#_6-模型选择建议" aria-label="Permalink to &quot;6. 模型选择建议&quot;">​</a></h3><ul><li><strong>中文场景</strong>：BGE、M3E、text2vec</li><li><strong>英文场景</strong>：OpenAI Embeddings、Sentence-BERT</li><li><strong>多语言场景</strong>：multilingual-e5、paraphrase-multilingual</li></ul><h2 id="实际应用案例" tabindex="-1">实际应用案例 <a class="header-anchor" href="#实际应用案例" aria-label="Permalink to &quot;实际应用案例&quot;">​</a></h2><h3 id="案例-1-智能客服系统" tabindex="-1">案例 1：智能客服系统 <a class="header-anchor" href="#案例-1-智能客服系统" aria-label="Permalink to &quot;案例 1：智能客服系统&quot;">​</a></h3><p><strong>应用</strong>：</p><ul><li>将历史对话和知识库转换为向量</li><li>用户提问时，检索最相关的历史回答</li><li>提供快速、准确的客服响应</li></ul><p><strong>效果</strong>：</p><ul><li>响应速度提升 80%</li><li>准确率提升 60%</li><li>用户满意度显著提高</li></ul><h3 id="案例-2-电商推荐系统" tabindex="-1">案例 2：电商推荐系统 <a class="header-anchor" href="#案例-2-电商推荐系统" aria-label="Permalink to &quot;案例 2：电商推荐系统&quot;">​</a></h3><p><strong>应用</strong>：</p><ul><li>将商品描述、用户行为转换为向量</li><li>通过向量相似度计算商品推荐</li><li>实现个性化推荐</li></ul><p><strong>效果</strong>：</p><ul><li>推荐点击率提升 40%</li><li>转化率提升 25%</li><li>用户停留时间增加</li></ul><h3 id="案例-3-代码搜索与推荐" tabindex="-1">案例 3：代码搜索与推荐 <a class="header-anchor" href="#案例-3-代码搜索与推荐" aria-label="Permalink to &quot;案例 3：代码搜索与推荐&quot;">​</a></h3><p><strong>应用</strong>：</p><ul><li>将代码片段转换为向量</li><li>支持语义化代码搜索</li><li>推荐相似代码示例</li></ul><p><strong>效果</strong>：</p><ul><li>代码搜索准确率提升</li><li>开发效率显著提高</li></ul><h2 id="挑战与限制" tabindex="-1">挑战与限制 <a class="header-anchor" href="#挑战与限制" aria-label="Permalink to &quot;挑战与限制&quot;">​</a></h2><h3 id="_1-计算成本" tabindex="-1">1. 计算成本 <a class="header-anchor" href="#_1-计算成本" aria-label="Permalink to &quot;1. 计算成本&quot;">​</a></h3><ul><li>大规模向量计算需要大量计算资源</li><li>向量数据库存储成本较高</li><li>需要优化算法和硬件加速</li></ul><h3 id="_2-语义理解局限" tabindex="-1">2. 语义理解局限 <a class="header-anchor" href="#_2-语义理解局限" aria-label="Permalink to &quot;2. 语义理解局限&quot;">​</a></h3><ul><li>无法完全理解上下文和语境</li><li>对专业术语、新词处理能力有限</li><li>可能存在语义偏差</li></ul><h3 id="_3-多语言支持" tabindex="-1">3. 多语言支持 <a class="header-anchor" href="#_3-多语言支持" aria-label="Permalink to &quot;3. 多语言支持&quot;">​</a></h3><ul><li>不同语言的嵌入质量差异较大</li><li>跨语言检索效果有待提升</li><li>需要针对性的模型优化</li></ul><h3 id="_4-领域适应性" tabindex="-1">4. 领域适应性 <a class="header-anchor" href="#_4-领域适应性" aria-label="Permalink to &quot;4. 领域适应性&quot;">​</a></h3><ul><li>通用模型在特定领域表现可能不佳</li><li>需要领域数据微调</li><li>知识更新需要重新训练或增量学习</li></ul><h2 id="未来发展趋势" tabindex="-1">未来发展趋势 <a class="header-anchor" href="#未来发展趋势" aria-label="Permalink to &quot;未来发展趋势&quot;">​</a></h2><h3 id="_1-更高质量的嵌入模型" tabindex="-1">1. 更高质量的嵌入模型 <a class="header-anchor" href="#_1-更高质量的嵌入模型" aria-label="Permalink to &quot;1. 更高质量的嵌入模型&quot;">​</a></h3><ul><li>更大规模的预训练</li><li>更好的训练策略</li><li>更强的语义理解能力</li></ul><h3 id="_2-多模态统一嵌入" tabindex="-1">2. 多模态统一嵌入 <a class="header-anchor" href="#_2-多模态统一嵌入" aria-label="Permalink to &quot;2. 多模态统一嵌入&quot;">​</a></h3><ul><li>文本、图像、音频统一表示</li><li>跨模态检索能力提升</li><li>多模态应用场景扩展</li></ul><h3 id="_3-长文本嵌入优化" tabindex="-1">3. 长文本嵌入优化 <a class="header-anchor" href="#_3-长文本嵌入优化" aria-label="Permalink to &quot;3. 长文本嵌入优化&quot;">​</a></h3><ul><li>支持更长文档的嵌入</li><li>保持长距离依赖关系</li><li>提升长文本检索质量</li></ul><h3 id="_4-实时更新与增量学习" tabindex="-1">4. 实时更新与增量学习 <a class="header-anchor" href="#_4-实时更新与增量学习" aria-label="Permalink to &quot;4. 实时更新与增量学习&quot;">​</a></h3><ul><li>支持知识实时更新</li><li>增量学习新知识</li><li>减少重新训练成本</li></ul><h3 id="_5-可解释性增强" tabindex="-1">5. 可解释性增强 <a class="header-anchor" href="#_5-可解释性增强" aria-label="Permalink to &quot;5. 可解释性增强&quot;">​</a></h3><ul><li>理解向量各维度的含义</li><li>可视化嵌入空间</li><li>提升模型可解释性</li></ul><h2 id="总结" tabindex="-1">总结 <a class="header-anchor" href="#总结" aria-label="Permalink to &quot;总结&quot;">​</a></h2><p>向量嵌入是大语言模型应用中的核心技术，它将离散的文本数据转换为连续的向量表示，使得计算机能够理解和处理语义信息。通过合理选择模型、优化计算策略、结合具体应用场景，向量嵌入能够在 RAG、推荐系统、语义搜索等应用中发挥重要作用。</p><p>随着模型技术的不断发展和优化，向量嵌入的质量和效率将持续提升，为更多创新应用提供强大的技术支撑。</p><hr><blockquote><p>💡 <strong>参考资源</strong>：</p><ul><li><a href="https://huggingface.co/models?pipeline_tag=sentence-similarity" target="_blank" rel="noreferrer">Hugging Face Embeddings</a></li><li><a href="https://platform.openai.com/docs/guides/embeddings" target="_blank" rel="noreferrer">OpenAI Embeddings Guide</a></li><li><a href="https://www.sbert.net/" target="_blank" rel="noreferrer">Sentence Transformers Documentation</a></li></ul></blockquote></div></div></main><footer class="VPDocFooter" data-v-39a288b8 data-v-e257564d><!--[--><!--]--><div class="edit-info" data-v-e257564d><!----><div class="last-updated" data-v-e257564d><p class="VPLastUpdated" data-v-e257564d data-v-e98dd255>最后更新: <time datetime="2026-01-27T16:32:18.000Z" data-v-e98dd255></time></p></div></div><nav class="prev-next" aria-labelledby="doc-footer-aria-label" data-v-e257564d><span class="visually-hidden" id="doc-footer-aria-label" data-v-e257564d>Pager</span><div class="pager" data-v-e257564d><a class="VPLink link pager-link prev" href="/ai/latest-developments.html" data-v-e257564d><!--[--><span class="desc" data-v-e257564d>Previous page</span><span class="title" data-v-e257564d>AI 最新进展</span><!--]--></a></div><div class="pager" data-v-e257564d><a class="VPLink link pager-link next" href="/ai/prompt-engineering.html" data-v-e257564d><!--[--><span class="desc" data-v-e257564d>Next page</span><span class="title" data-v-e257564d>Prompt 工程：大语言模型交互的艺术</span><!--]--></a></div></nav></footer><!--[--><!--]--></div></div></div><!--[--><!--]--></div></div><footer class="VPFooter has-sidebar" data-v-5d98c3a5 data-v-e315a0ad><div class="container" data-v-e315a0ad><p class="message" data-v-e315a0ad>Released under the MIT License.</p><p class="copyright" data-v-e315a0ad>Copyright © 2025-present Wang</p></div></footer><!--[--><!--]--></div></div>
    <script>window.__VP_HASH_MAP__=JSON.parse("{\"30-day-algorithm_day01.md\":\"DJlrTW85\",\"30-day-algorithm_day02.md\":\"Dt6lz8Fc\",\"30-day-algorithm_day03.md\":\"C8CYYJN2\",\"30-day-algorithm_day04.md\":\"CIafvmGo\",\"30-day-algorithm_day05.md\":\"DDb39CdP\",\"30-day-algorithm_day06.md\":\"CphoMELq\",\"30-day-algorithm_day07.md\":\"idyOgveA\",\"30-day-algorithm_day08.md\":\"BqP5nt1Z\",\"30-day-algorithm_day09.md\":\"BWOTMfA1\",\"30-day-algorithm_day10.md\":\"DZl33HJS\",\"30-day-algorithm_day11.md\":\"BPzMhiw7\",\"30-day-algorithm_day12.md\":\"Boct6D98\",\"30-day-algorithm_day13.md\":\"DGn8ZlF-\",\"30-day-algorithm_day14.md\":\"C-SHPLdV\",\"30-day-algorithm_day15.md\":\"Cz7nTGRC\",\"30-day-algorithm_day16.md\":\"BDGPC0op\",\"30-day-algorithm_day17.md\":\"WhzXDkqg\",\"30-day-algorithm_day18.md\":\"DEG6_584\",\"30-day-algorithm_index.md\":\"CrOrYqZ7\",\"about.md\":\"D0LY0p4H\",\"ai_index.md\":\"AzYtRydo\",\"ai_latest-developments.md\":\"CjHBs9rD\",\"ai_living-files-theory.md\":\"BRQ0Nv0V\",\"ai_openclaw-memory-architecture.md\":\"BGeHzlKh\",\"ai_prompt-engineering.md\":\"BedomlbR\",\"ai_vector-embedding.md\":\"DM0LQQU-\",\"index.md\":\"v22f_hT9\",\"interview_concurrency-design-patterns.md\":\"DxnsdowV\",\"interview_concurrency-juc.md\":\"Dpr9oUf7\",\"interview_concurrency-thread-basics.md\":\"DJJkg0TL\",\"interview_index.md\":\"qAARLP1_\",\"interview_java-basics-collections.md\":\"VpI_8GVN\",\"interview_java-basics-io-nio.md\":\"Du0H0EyS\",\"interview_java-basics-language-features.md\":\"DClYFwOc\",\"interview_java-interview-outline.md\":\"BAXZvNfl\",\"interview_jvm-basics.md\":\"XtJLWO_P\",\"others_base64-explanation.md\":\"BVHwGglv\",\"others_binary_addition.md\":\"1i4EhbLF\",\"others_cosine.md\":\"BaYaNYiv\",\"others_dp.md\":\"CfuxKDds\",\"others_index.md\":\"BKpa52X1\",\"others_leetcode100.md\":\"Chr2TmaZ\",\"others_mcp.md\":\"BD2BPzED\",\"others_mysql.md\":\"3QV6-5YP\",\"others_quick-sort-java.md\":\"C4XHVTcv\",\"others_slow-sql-governance.md\":\"THd8EZjV\",\"others_sort.md\":\"dd-WI5oX\",\"others_sword-offer.md\":\"0z7bU_J4\",\"others_unionfind.md\":\"CbF-D-h6\"}");window.__VP_SITE_DATA__=JSON.parse("{\"lang\":\"zh-CN\",\"dir\":\"ltr\",\"title\":\"Yabin's Tech Journey\",\"description\":\"算法、java、AI\",\"base\":\"/\",\"head\":[],\"router\":{\"prefetchLinks\":true},\"appearance\":false,\"themeConfig\":{\"siteTitle\":\"Yabin's Tech Journey\",\"nav\":[{\"text\":\"🚀 30天刷题计划\",\"link\":\"/30-day-algorithm/\"},{\"text\":\"☕ Java 面试\",\"link\":\"/interview/\"},{\"text\":\"🤖 AI 探索\",\"link\":\"/ai/\"},{\"text\":\"📝 其他文章\",\"link\":\"/others/\"}],\"sidebar\":{\"/30-day-algorithm/\":{\"base\":\"/30-day-algorithm/\",\"items\":[{\"text\":\"Day 1 - 数组\",\"link\":\"day01\"},{\"text\":\"Day 2 - 数组进阶\",\"link\":\"day02\"},{\"text\":\"Day 3 - 链表专题\",\"link\":\"day03\"},{\"text\":\"Day 4 - 链表进阶\",\"link\":\"day04\"},{\"text\":\"Day 5 - 哈希表基础\",\"link\":\"day05\"},{\"text\":\"Day 6 - 哈希表进阶\",\"link\":\"day06\"},{\"text\":\"Day 7 - 字符串基础\",\"link\":\"day07\"},{\"text\":\"Day 8 - 字符串进阶\",\"link\":\"day08\"},{\"text\":\"Day 9 - 双指针基础\",\"link\":\"day09\"},{\"text\":\"Day 10 - 双指针进阶\",\"link\":\"day10\"},{\"text\":\"Day 11 - 滑动窗口基础\",\"link\":\"day11\"},{\"text\":\"Day 12 - 滑动窗口进阶\",\"link\":\"day12\"},{\"text\":\"Day 13 - 栈\",\"link\":\"day13\"},{\"text\":\"Day 14 - 队列\",\"link\":\"day14\"},{\"text\":\"Day 15 - 二叉树\",\"link\":\"day15\"},{\"text\":\"Day 16 - 二叉树\",\"link\":\"day16\"},{\"text\":\"Day 17 - 二叉树\",\"link\":\"day17\"},{\"text\":\"Day 18 - 二叉搜索树\",\"link\":\"day18\"}]},\"/interview/\":[{\"text\":\"Java程序员面试学习大纲\",\"link\":\"/interview/java-interview-outline\"},{\"text\":\"☕ Java 面试学习\",\"link\":\"/interview/\"},{\"text\":\"Java基础\",\"collapsed\":false,\"items\":[{\"text\":\"语言特性\",\"link\":\"/interview/java-basics-language-features\"},{\"text\":\"集合框架\",\"link\":\"/interview/java-basics-collections\"},{\"text\":\"IO/NIO\",\"link\":\"/interview/java-basics-io-nio\"}]},{\"text\":\"并发编程\",\"collapsed\":false,\"items\":[{\"text\":\"线程基础\",\"link\":\"/interview/concurrency-thread-basics\"},{\"text\":\"JUC包\",\"link\":\"/interview/concurrency-juc\"},{\"text\":\"并发设计模式\",\"link\":\"/interview/concurrency-design-patterns\"}]},{\"text\":\"JVM\",\"collapsed\":false,\"items\":[{\"text\":\"内存模型、GC与类加载\",\"link\":\"/interview/jvm-basics\"}]}],\"/ai/\":{\"base\":\"/ai/\",\"items\":[{\"text\":\"AI 最新进展\",\"link\":\"latest-developments\"},{\"text\":\"大模型中的向量嵌入（Vector Embedding）\",\"link\":\"vector-embedding\"},{\"text\":\"Prompt 工程：大语言模型交互的艺术\",\"link\":\"prompt-engineering\"},{\"text\":\"活文件理论 (The \\\"Living Files\\\" Theory)\",\"link\":\"living-files-theory\"},{\"text\":\"🧠 OpenClaw 记忆系统架构\",\"link\":\"openclaw-memory-architecture\"}]},\"/others/\":{\"base\":\"/others/\",\"items\":[{\"text\":\"Base64 编码详解\",\"link\":\"base64-explanation\"},{\"text\":\"二进制加法算法\",\"link\":\"binary_addition\"},{\"text\":\"余弦相似度\",\"link\":\"cosine\"},{\"text\":\"动态规划\",\"link\":\"dp\"},{\"text\":\"100 道 LeetCode 算法题\",\"link\":\"leetCode100\"},{\"text\":\"MCP（模型上下文协议）详解\",\"link\":\"mcp\"},{\"text\":\"Innodb是如何实现事务的\",\"link\":\"mysql\"},{\"text\":\"快速排序算法 - Java实现\",\"link\":\"quick-sort-java\"},{\"text\":\"慢SQL治理总结\",\"link\":\"slow-sql-governance\"},{\"text\":\"sort\",\"link\":\"sort\"},{\"text\":\"剑指Offer算法学习笔记\",\"link\":\"sword-offer\"},{\"text\":\"并查集(Disjoint Set Union)\",\"link\":\"unionFind\"}]},\"/interview\":[{\"text\":\"Java程序员面试学习大纲\",\"link\":\"/interview/java-interview-outline\"},{\"text\":\"☕ Java 面试学习\",\"link\":\"/interview/\"},{\"text\":\"Java基础\",\"collapsed\":false,\"items\":[{\"text\":\"语言特性\",\"link\":\"/interview/java-basics-language-features\"},{\"text\":\"集合框架\",\"link\":\"/interview/java-basics-collections\"},{\"text\":\"IO/NIO\",\"link\":\"/interview/java-basics-io-nio\"}]},{\"text\":\"并发编程\",\"collapsed\":false,\"items\":[{\"text\":\"线程基础\",\"link\":\"/interview/concurrency-thread-basics\"},{\"text\":\"JUC包\",\"link\":\"/interview/concurrency-juc\"},{\"text\":\"并发设计模式\",\"link\":\"/interview/concurrency-design-patterns\"}]},{\"text\":\"JVM\",\"collapsed\":false,\"items\":[{\"text\":\"内存模型、GC与类加载\",\"link\":\"/interview/jvm-basics\"}]}],\"/ai\":{\"base\":\"/ai/\",\"items\":[{\"text\":\"AI 最新进展\",\"link\":\"latest-developments\"},{\"text\":\"大模型中的向量嵌入（Vector Embedding）\",\"link\":\"vector-embedding\"},{\"text\":\"Prompt 工程：大语言模型交互的艺术\",\"link\":\"prompt-engineering\"},{\"text\":\"活文件理论 (The \\\"Living Files\\\" Theory)\",\"link\":\"living-files-theory\"},{\"text\":\"🧠 OpenClaw 记忆系统架构\",\"link\":\"openclaw-memory-architecture\"}]},\"/others\":{\"base\":\"/others/\",\"items\":[{\"text\":\"Base64 编码详解\",\"link\":\"base64-explanation\"},{\"text\":\"二进制加法算法\",\"link\":\"binary_addition\"},{\"text\":\"余弦相似度\",\"link\":\"cosine\"},{\"text\":\"动态规划\",\"link\":\"dp\"},{\"text\":\"100 道 LeetCode 算法题\",\"link\":\"leetCode100\"},{\"text\":\"MCP（模型上下文协议）详解\",\"link\":\"mcp\"},{\"text\":\"Innodb是如何实现事务的\",\"link\":\"mysql\"},{\"text\":\"快速排序算法 - Java实现\",\"link\":\"quick-sort-java\"},{\"text\":\"慢SQL治理总结\",\"link\":\"slow-sql-governance\"},{\"text\":\"sort\",\"link\":\"sort\"},{\"text\":\"剑指Offer算法学习笔记\",\"link\":\"sword-offer\"},{\"text\":\"并查集(Disjoint Set Union)\",\"link\":\"unionFind\"}]}},\"socialLinks\":[{\"icon\":\"github\",\"link\":\"https://github.com/yabwang\"}],\"search\":{\"provider\":\"local\"},\"lastUpdated\":{\"text\":\"最后更新\"},\"footer\":{\"message\":\"Released under the MIT License.\",\"copyright\":\"Copyright © 2025-present Wang\"},\"outline\":{\"level\":[2,3],\"label\":\"目录\"}},\"locales\":{},\"scrollOffset\":134,\"cleanUrls\":false}");</script>
    
  </body>
</html>