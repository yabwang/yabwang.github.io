import{_ as a,c as s,o as l,ag as n}from"./chunks/framework.DK1-H3E1.js";const g=JSON.parse('{"title":"大模型中的向量嵌入（Vector Embedding）","description":"","frontmatter":{"order":3},"headers":[],"relativePath":"ai/vector-embedding.md","filePath":"ai/vector-embedding.md","lastUpdated":1769531538000}'),t={name:"ai/vector-embedding.md"};function e(h,i,r,o,p,k){return l(),s("div",null,i[0]||(i[0]=[n(`<h1 id="大模型中的向量嵌入-vector-embedding" tabindex="-1">大模型中的向量嵌入（Vector Embedding） <a class="header-anchor" href="#大模型中的向量嵌入-vector-embedding" aria-label="Permalink to &quot;大模型中的向量嵌入（Vector Embedding）&quot;">​</a></h1><blockquote><p>📅 最后更新：2025年1月<br> 🎯 主题：向量嵌入原理、应用与实践</p></blockquote><h2 id="引言" tabindex="-1">引言 <a class="header-anchor" href="#引言" aria-label="Permalink to &quot;引言&quot;">​</a></h2><p>向量嵌入（Vector Embedding）是大语言模型中的核心技术之一，它将文本、图像等离散数据转换为连续的向量表示，使得计算机能够理解和处理语义信息。在 RAG（检索增强生成）、推荐系统、语义搜索等应用中，向量嵌入发挥着至关重要的作用。</p><h2 id="什么是向量嵌入" tabindex="-1">什么是向量嵌入 <a class="header-anchor" href="#什么是向量嵌入" aria-label="Permalink to &quot;什么是向量嵌入&quot;">​</a></h2><h3 id="基本定义" tabindex="-1">基本定义 <a class="header-anchor" href="#基本定义" aria-label="Permalink to &quot;基本定义&quot;">​</a></h3><p><strong>向量嵌入（Embedding）</strong> 是一种将高维且通常离散的输入数据（如单词、短语、句子、文档等）映射到低维连续向量空间中的技术。简单来说，它是将文本转换成能表达语义信息的浮点数向量。</p><h3 id="为什么需要向量嵌入" tabindex="-1">为什么需要向量嵌入 <a class="header-anchor" href="#为什么需要向量嵌入" aria-label="Permalink to &quot;为什么需要向量嵌入&quot;">​</a></h3><ol><li><strong>语义表示</strong>：将文本转换为数值向量，使计算机能够理解和比较文本的语义</li><li><strong>相似度计算</strong>：通过向量间的数学距离反映文本间的语义相关性</li><li><strong>降维处理</strong>：将高维稀疏的文本数据转换为低维稠密的向量表示</li><li><strong>计算效率</strong>：向量运算比文本匹配更高效，支持大规模数据处理</li></ol><h2 id="核心原理与工作机制" tabindex="-1">核心原理与工作机制 <a class="header-anchor" href="#核心原理与工作机制" aria-label="Permalink to &quot;核心原理与工作机制&quot;">​</a></h2><h3 id="从文本到向量的转换过程" tabindex="-1">从文本到向量的转换过程 <a class="header-anchor" href="#从文本到向量的转换过程" aria-label="Permalink to &quot;从文本到向量的转换过程&quot;">​</a></h3><p>向量嵌入的生成过程通常包括以下步骤：</p><ol><li><strong>文本预处理</strong>：对输入文本进行清洗、分词等预处理</li><li><strong>Token 化</strong>：将文本拆分成 token（词或子词单元）</li><li><strong>编号映射</strong>：将 token 映射成数字编号（Token ID）</li><li><strong>Embedding 查询</strong>：根据编号从 Embedding 矩阵中查询对应的向量</li><li><strong>向量聚合</strong>：对于多个 token，通过平均、池化等方式生成最终向量</li></ol><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>文本 → Token → 编号 → Embedding矩阵 → 向量</span></span>
<span class="line"><span>&quot;Hello World&quot; → [&quot;Hello&quot;, &quot;World&quot;] → [101, 102] → [[0.1, 0.2, ...], [0.3, 0.4, ...]] → [0.2, 0.3, ...]</span></span></code></pre></div><h3 id="语义相似性原理" tabindex="-1">语义相似性原理 <a class="header-anchor" href="#语义相似性原理" aria-label="Permalink to &quot;语义相似性原理&quot;">​</a></h3><p>在 Embedding 空间中，语义相似的数据点具有相近的向量表示：</p><ul><li><strong>相似文本</strong>：如&quot;猫&quot;和&quot;狗&quot;的向量距离很近</li><li><strong>不相关文本</strong>：如&quot;房子&quot;和&quot;你好&quot;的向量距离很远</li><li><strong>语义关系</strong>：通过向量运算可以捕捉同义词、反义词、上下位关系等</li></ul><h3 id="关键技术" tabindex="-1">关键技术 <a class="header-anchor" href="#关键技术" aria-label="Permalink to &quot;关键技术&quot;">​</a></h3><p>现代向量嵌入技术主要基于：</p><ul><li><strong>Transformer 架构</strong>：采用自注意力机制捕捉序列中的长距离依赖关系</li><li><strong>预训练模型</strong>：在大规模语料上预训练，学习通用的语义表示</li><li><strong>并行计算</strong>：通过 GPU 加速，支持大规模向量计算</li><li><strong>内存优化</strong>：采用量化、压缩等技术降低存储和计算成本</li></ul><h2 id="技术实现" tabindex="-1">技术实现 <a class="header-anchor" href="#技术实现" aria-label="Permalink to &quot;技术实现&quot;">​</a></h2><h3 id="基于-transformer-的-embedding-模型" tabindex="-1">基于 Transformer 的 Embedding 模型 <a class="header-anchor" href="#基于-transformer-的-embedding-模型" aria-label="Permalink to &quot;基于 Transformer 的 Embedding 模型&quot;">​</a></h3><p>主流的 Embedding 模型通常基于 Transformer 架构：</p><h4 id="_1-bert-系列" tabindex="-1">1. BERT 系列 <a class="header-anchor" href="#_1-bert-系列" aria-label="Permalink to &quot;1. BERT 系列&quot;">​</a></h4><ul><li><strong>BERT</strong>：双向编码器，适合理解任务</li><li><strong>RoBERTa</strong>：优化训练策略的 BERT</li><li><strong>ALBERT</strong>：参数共享，降低模型大小</li></ul><h4 id="_2-gpt-系列" tabindex="-1">2. GPT 系列 <a class="header-anchor" href="#_2-gpt-系列" aria-label="Permalink to &quot;2. GPT 系列&quot;">​</a></h4><ul><li><strong>GPT</strong>：自回归模型，适合生成任务</li><li><strong>GPT-3/4</strong>：大规模预训练模型</li></ul><h4 id="_3-专门优化的-embedding-模型" tabindex="-1">3. 专门优化的 Embedding 模型 <a class="header-anchor" href="#_3-专门优化的-embedding-模型" aria-label="Permalink to &quot;3. 专门优化的 Embedding 模型&quot;">​</a></h4><ul><li><strong>Sentence-BERT (SBERT)</strong>：专门为句子嵌入优化</li><li><strong>OpenAI Embeddings</strong>：text-embedding-ada-002 等</li><li><strong>BGE (BAAI General Embedding)</strong>：中文优化的嵌入模型</li></ul><h3 id="训练方法" tabindex="-1">训练方法 <a class="header-anchor" href="#训练方法" aria-label="Permalink to &quot;训练方法&quot;">​</a></h3><h4 id="_1-预训练-pre-training" tabindex="-1">1. 预训练（Pre-training） <a class="header-anchor" href="#_1-预训练-pre-training" aria-label="Permalink to &quot;1. 预训练（Pre-training）&quot;">​</a></h4><ul><li>在大规模无标注语料上训练</li><li>学习通用的语言表示能力</li><li>常见任务：掩码语言模型（MLM）、下一句预测（NSP）</li></ul><h4 id="_2-微调-fine-tuning" tabindex="-1">2. 微调（Fine-tuning） <a class="header-anchor" href="#_2-微调-fine-tuning" aria-label="Permalink to &quot;2. 微调（Fine-tuning）&quot;">​</a></h4><ul><li>在特定任务数据上微调</li><li>提升特定领域的表现</li><li>支持监督学习和对比学习</li></ul><h4 id="_3-对比学习-contrastive-learning" tabindex="-1">3. 对比学习（Contrastive Learning） <a class="header-anchor" href="#_3-对比学习-contrastive-learning" aria-label="Permalink to &quot;3. 对比学习（Contrastive Learning）&quot;">​</a></h4><ul><li>通过正负样本对比学习</li><li>拉近相似文本，推远不相似文本</li><li>提升嵌入质量</li></ul><h2 id="主要应用场景" tabindex="-1">主要应用场景 <a class="header-anchor" href="#主要应用场景" aria-label="Permalink to &quot;主要应用场景&quot;">​</a></h2><h3 id="_1-rag-检索增强生成" tabindex="-1">1. RAG（检索增强生成） <a class="header-anchor" href="#_1-rag-检索增强生成" aria-label="Permalink to &quot;1. RAG（检索增强生成）&quot;">​</a></h3><p><strong>应用流程</strong>：</p><ol><li>将文档库转换为向量并存储到向量数据库</li><li>将用户查询转换为向量</li><li>通过向量相似度检索相关文档片段</li><li>将检索到的文档作为上下文输入大模型生成答案</li></ol><p><strong>优势</strong>：</p><ul><li>突破模型上下文长度限制</li><li>提供实时、准确的外部知识</li><li>支持知识更新和领域定制</li></ul><h3 id="_2-推荐系统" tabindex="-1">2. 推荐系统 <a class="header-anchor" href="#_2-推荐系统" aria-label="Permalink to &quot;2. 推荐系统&quot;">​</a></h3><p><strong>应用方式</strong>：</p><ul><li>将用户行为、商品属性转换为向量</li><li>通过向量相似度计算用户-商品匹配度</li><li>实现个性化推荐</li></ul><p><strong>效果</strong>：</p><ul><li>提升推荐准确性</li><li>发现潜在兴趣关联</li><li>支持冷启动问题</li></ul><h3 id="_3-语义搜索" tabindex="-1">3. 语义搜索 <a class="header-anchor" href="#_3-语义搜索" aria-label="Permalink to &quot;3. 语义搜索&quot;">​</a></h3><p><strong>特点</strong>：</p><ul><li>理解查询意图，而非简单的关键词匹配</li><li>支持同义词、相关词检索</li><li>提升搜索相关性</li></ul><h3 id="_4-文本相似度与聚类" tabindex="-1">4. 文本相似度与聚类 <a class="header-anchor" href="#_4-文本相似度与聚类" aria-label="Permalink to &quot;4. 文本相似度与聚类&quot;">​</a></h3><p><strong>应用</strong>：</p><ul><li><strong>相似度判断</strong>：判断两段文本的语义相似度</li><li><strong>文本聚类</strong>：将相似文本归为一类</li><li><strong>主题发现</strong>：通过聚类发现文本主题</li><li><strong>异常检测</strong>：识别与正常模式差异较大的文本</li></ul><h3 id="_5-多模态应用" tabindex="-1">5. 多模态应用 <a class="header-anchor" href="#_5-多模态应用" aria-label="Permalink to &quot;5. 多模态应用&quot;">​</a></h3><p><strong>扩展</strong>：</p><ul><li><strong>图像嵌入</strong>：将图像转换为向量</li><li><strong>跨模态检索</strong>：文本-图像、图像-图像检索</li><li><strong>统一表示</strong>：文本、图像、音频统一到同一向量空间</li></ul><h2 id="相似度计算" tabindex="-1">相似度计算 <a class="header-anchor" href="#相似度计算" aria-label="Permalink to &quot;相似度计算&quot;">​</a></h2><p>向量嵌入的相似度计算是应用的核心，常用的方法包括：</p><h3 id="_1-余弦相似度-cosine-similarity" tabindex="-1">1. 余弦相似度（Cosine Similarity） <a class="header-anchor" href="#_1-余弦相似度-cosine-similarity" aria-label="Permalink to &quot;1. 余弦相似度（Cosine Similarity）&quot;">​</a></h3><p>最常用的相似度计算方法，衡量向量方向的相似性：</p><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> cosine_similarity</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(vec1, vec2):</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">    &quot;&quot;&quot;</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">    计算两个向量的余弦相似度</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">    返回值范围：[-1, 1]，1 表示完全相同，-1 表示完全相反</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">    &quot;&quot;&quot;</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    dot_product </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> sum</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(a </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">*</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> b </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> a, b </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> zip</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(vec1, vec2))</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    norm1 </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> sum</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(a </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">*</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> a </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> a </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> vec1) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">**</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 0.5</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    norm2 </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> sum</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(b </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">*</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> b </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> b </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> vec2) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">**</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 0.5</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    </span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    if</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> norm1 </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">==</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 0</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> or</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> norm2 </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">==</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">        return</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 0.0</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    </span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    return</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> dot_product </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">/</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (norm1 </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">*</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> norm2)</span></span></code></pre></div><p><strong>特点</strong>：</p><ul><li>不受向量长度影响，只关注方向</li><li>适合高维稀疏向量</li><li>计算效率高</li></ul><h3 id="_2-欧氏距离-euclidean-distance" tabindex="-1">2. 欧氏距离（Euclidean Distance） <a class="header-anchor" href="#_2-欧氏距离-euclidean-distance" aria-label="Permalink to &quot;2. 欧氏距离（Euclidean Distance）&quot;">​</a></h3><p>衡量向量在空间中的实际距离：</p><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> numpy </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">as</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> np</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> euclidean_distance</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(vec1, vec2):</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">    &quot;&quot;&quot;</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">    计算两个向量的欧氏距离</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">    距离越小，相似度越高</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">    &quot;&quot;&quot;</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    return</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> np.sqrt(np.sum((np.array(vec1) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> np.array(vec2)) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">**</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">))</span></span></code></pre></div><p><strong>特点</strong>：</p><ul><li>直观理解：空间中的直线距离</li><li>受向量长度影响</li><li>适合低维稠密向量</li></ul><h3 id="_3-点积-dot-product" tabindex="-1">3. 点积（Dot Product） <a class="header-anchor" href="#_3-点积-dot-product" aria-label="Permalink to &quot;3. 点积（Dot Product）&quot;">​</a></h3><p>向量内积，简单高效：</p><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> dot_product</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(vec1, vec2):</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">    &quot;&quot;&quot;</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">    计算两个向量的点积</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">    值越大，相似度越高（通常需要归一化）</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">    &quot;&quot;&quot;</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    return</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> sum</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(a </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">*</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> b </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> a, b </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> zip</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(vec1, vec2))</span></span></code></pre></div><p><strong>特点</strong>：</p><ul><li>计算最快</li><li>需要向量归一化才能准确反映相似度</li><li>常用于大规模检索场景</li></ul><h3 id="_4-曼哈顿距离-manhattan-distance" tabindex="-1">4. 曼哈顿距离（Manhattan Distance） <a class="header-anchor" href="#_4-曼哈顿距离-manhattan-distance" aria-label="Permalink to &quot;4. 曼哈顿距离（Manhattan Distance）&quot;">​</a></h3><p>L1 距离，适合某些特定场景：</p><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> manhattan_distance</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(vec1, vec2):</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">    &quot;&quot;&quot;</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">    计算两个向量的曼哈顿距离</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">    &quot;&quot;&quot;</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    return</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> sum</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">abs</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(a </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> b) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> a, b </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> zip</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(vec1, vec2))</span></span></code></pre></div><h2 id="工具与框架" tabindex="-1">工具与框架 <a class="header-anchor" href="#工具与框架" aria-label="Permalink to &quot;工具与框架&quot;">​</a></h2><h3 id="_1-hugging-face-transformers" tabindex="-1">1. Hugging Face Transformers <a class="header-anchor" href="#_1-hugging-face-transformers" aria-label="Permalink to &quot;1. Hugging Face Transformers&quot;">​</a></h3><p><strong>特点</strong>：</p><ul><li>提供丰富的预训练 Embedding 模型</li><li>简单易用的 API</li><li>支持多种模型架构</li></ul><p><strong>示例</strong>：</p><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">from</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> sentence_transformers </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> SentenceTransformer</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">model </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> SentenceTransformer(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;all-MiniLM-L6-v2&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">embeddings </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> model.encode([</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;Hello world&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;How are you?&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">])</span></span></code></pre></div><h3 id="_2-openai-embeddings-api" tabindex="-1">2. OpenAI Embeddings API <a class="header-anchor" href="#_2-openai-embeddings-api" aria-label="Permalink to &quot;2. OpenAI Embeddings API&quot;">​</a></h3><p><strong>特点</strong>：</p><ul><li>云端服务，无需本地部署</li><li>高质量嵌入模型（text-embedding-ada-002, text-embedding-3-small/large）</li><li>支持多语言</li></ul><p><strong>示例</strong>：</p><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> openai</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">response </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> openai.Embedding.create(</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">    input</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;Your text here&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">    model</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;text-embedding-ada-002&quot;</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">embedding </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> response[</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;data&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">][</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">][</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;embedding&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]</span></span></code></pre></div><h3 id="_3-langchain" tabindex="-1">3. LangChain <a class="header-anchor" href="#_3-langchain" aria-label="Permalink to &quot;3. LangChain&quot;">​</a></h3><p><strong>特点</strong>：</p><ul><li>集成多种 Embedding 模型</li><li>提供向量存储和检索功能</li><li>支持 RAG 应用开发</li></ul><h3 id="_4-向量数据库" tabindex="-1">4. 向量数据库 <a class="header-anchor" href="#_4-向量数据库" aria-label="Permalink to &quot;4. 向量数据库&quot;">​</a></h3><p><strong>主流选择</strong>：</p><ul><li><strong>Pinecone</strong>：云端向量数据库，易于使用</li><li><strong>Weaviate</strong>：开源向量搜索引擎</li><li><strong>Milvus</strong>：高性能开源向量数据库</li><li><strong>Chroma</strong>：轻量级嵌入式向量数据库</li><li><strong>Qdrant</strong>：高性能向量搜索引擎</li></ul><h2 id="最佳实践" tabindex="-1">最佳实践 <a class="header-anchor" href="#最佳实践" aria-label="Permalink to &quot;最佳实践&quot;">​</a></h2><h3 id="_1-向量维度选择" tabindex="-1">1. 向量维度选择 <a class="header-anchor" href="#_1-向量维度选择" aria-label="Permalink to &quot;1. 向量维度选择&quot;">​</a></h3><ul><li><strong>低维（128-256）</strong>：计算快，适合大规模检索，但表达能力有限</li><li><strong>中维（384-512）</strong>：平衡性能和效果，推荐用于大多数场景</li><li><strong>高维（768-1536）</strong>：表达能力强，但计算成本高，适合精度要求高的场景</li></ul><h3 id="_2-相似度阈值设置" tabindex="-1">2. 相似度阈值设置 <a class="header-anchor" href="#_2-相似度阈值设置" aria-label="Permalink to &quot;2. 相似度阈值设置&quot;">​</a></h3><ul><li><strong>精确匹配</strong>：阈值 0.9-0.95</li><li><strong>相关匹配</strong>：阈值 0.7-0.85</li><li><strong>模糊匹配</strong>：阈值 0.5-0.7</li></ul><p>根据具体应用场景调整阈值，平衡召回率和准确率。</p><h3 id="_3-批量处理优化" tabindex="-1">3. 批量处理优化 <a class="header-anchor" href="#_3-批量处理优化" aria-label="Permalink to &quot;3. 批量处理优化&quot;">​</a></h3><ul><li>使用批量编码减少 API 调用</li><li>利用 GPU 加速向量计算</li><li>采用异步处理提升吞吐量</li></ul><h3 id="_4-向量归一化" tabindex="-1">4. 向量归一化 <a class="header-anchor" href="#_4-向量归一化" aria-label="Permalink to &quot;4. 向量归一化&quot;">​</a></h3><ul><li>对向量进行 L2 归一化，使余弦相似度等于点积</li><li>提升计算效率和数值稳定性</li></ul><h3 id="_5-混合检索策略" tabindex="-1">5. 混合检索策略 <a class="header-anchor" href="#_5-混合检索策略" aria-label="Permalink to &quot;5. 混合检索策略&quot;">​</a></h3><ul><li><strong>向量检索</strong>：语义相似度</li><li><strong>关键词检索</strong>：精确匹配</li><li><strong>混合检索</strong>：结合两者优势，提升检索质量</li></ul><h3 id="_6-模型选择建议" tabindex="-1">6. 模型选择建议 <a class="header-anchor" href="#_6-模型选择建议" aria-label="Permalink to &quot;6. 模型选择建议&quot;">​</a></h3><ul><li><strong>中文场景</strong>：BGE、M3E、text2vec</li><li><strong>英文场景</strong>：OpenAI Embeddings、Sentence-BERT</li><li><strong>多语言场景</strong>：multilingual-e5、paraphrase-multilingual</li></ul><h2 id="实际应用案例" tabindex="-1">实际应用案例 <a class="header-anchor" href="#实际应用案例" aria-label="Permalink to &quot;实际应用案例&quot;">​</a></h2><h3 id="案例-1-智能客服系统" tabindex="-1">案例 1：智能客服系统 <a class="header-anchor" href="#案例-1-智能客服系统" aria-label="Permalink to &quot;案例 1：智能客服系统&quot;">​</a></h3><p><strong>应用</strong>：</p><ul><li>将历史对话和知识库转换为向量</li><li>用户提问时，检索最相关的历史回答</li><li>提供快速、准确的客服响应</li></ul><p><strong>效果</strong>：</p><ul><li>响应速度提升 80%</li><li>准确率提升 60%</li><li>用户满意度显著提高</li></ul><h3 id="案例-2-电商推荐系统" tabindex="-1">案例 2：电商推荐系统 <a class="header-anchor" href="#案例-2-电商推荐系统" aria-label="Permalink to &quot;案例 2：电商推荐系统&quot;">​</a></h3><p><strong>应用</strong>：</p><ul><li>将商品描述、用户行为转换为向量</li><li>通过向量相似度计算商品推荐</li><li>实现个性化推荐</li></ul><p><strong>效果</strong>：</p><ul><li>推荐点击率提升 40%</li><li>转化率提升 25%</li><li>用户停留时间增加</li></ul><h3 id="案例-3-代码搜索与推荐" tabindex="-1">案例 3：代码搜索与推荐 <a class="header-anchor" href="#案例-3-代码搜索与推荐" aria-label="Permalink to &quot;案例 3：代码搜索与推荐&quot;">​</a></h3><p><strong>应用</strong>：</p><ul><li>将代码片段转换为向量</li><li>支持语义化代码搜索</li><li>推荐相似代码示例</li></ul><p><strong>效果</strong>：</p><ul><li>代码搜索准确率提升</li><li>开发效率显著提高</li></ul><h2 id="挑战与限制" tabindex="-1">挑战与限制 <a class="header-anchor" href="#挑战与限制" aria-label="Permalink to &quot;挑战与限制&quot;">​</a></h2><h3 id="_1-计算成本" tabindex="-1">1. 计算成本 <a class="header-anchor" href="#_1-计算成本" aria-label="Permalink to &quot;1. 计算成本&quot;">​</a></h3><ul><li>大规模向量计算需要大量计算资源</li><li>向量数据库存储成本较高</li><li>需要优化算法和硬件加速</li></ul><h3 id="_2-语义理解局限" tabindex="-1">2. 语义理解局限 <a class="header-anchor" href="#_2-语义理解局限" aria-label="Permalink to &quot;2. 语义理解局限&quot;">​</a></h3><ul><li>无法完全理解上下文和语境</li><li>对专业术语、新词处理能力有限</li><li>可能存在语义偏差</li></ul><h3 id="_3-多语言支持" tabindex="-1">3. 多语言支持 <a class="header-anchor" href="#_3-多语言支持" aria-label="Permalink to &quot;3. 多语言支持&quot;">​</a></h3><ul><li>不同语言的嵌入质量差异较大</li><li>跨语言检索效果有待提升</li><li>需要针对性的模型优化</li></ul><h3 id="_4-领域适应性" tabindex="-1">4. 领域适应性 <a class="header-anchor" href="#_4-领域适应性" aria-label="Permalink to &quot;4. 领域适应性&quot;">​</a></h3><ul><li>通用模型在特定领域表现可能不佳</li><li>需要领域数据微调</li><li>知识更新需要重新训练或增量学习</li></ul><h2 id="未来发展趋势" tabindex="-1">未来发展趋势 <a class="header-anchor" href="#未来发展趋势" aria-label="Permalink to &quot;未来发展趋势&quot;">​</a></h2><h3 id="_1-更高质量的嵌入模型" tabindex="-1">1. 更高质量的嵌入模型 <a class="header-anchor" href="#_1-更高质量的嵌入模型" aria-label="Permalink to &quot;1. 更高质量的嵌入模型&quot;">​</a></h3><ul><li>更大规模的预训练</li><li>更好的训练策略</li><li>更强的语义理解能力</li></ul><h3 id="_2-多模态统一嵌入" tabindex="-1">2. 多模态统一嵌入 <a class="header-anchor" href="#_2-多模态统一嵌入" aria-label="Permalink to &quot;2. 多模态统一嵌入&quot;">​</a></h3><ul><li>文本、图像、音频统一表示</li><li>跨模态检索能力提升</li><li>多模态应用场景扩展</li></ul><h3 id="_3-长文本嵌入优化" tabindex="-1">3. 长文本嵌入优化 <a class="header-anchor" href="#_3-长文本嵌入优化" aria-label="Permalink to &quot;3. 长文本嵌入优化&quot;">​</a></h3><ul><li>支持更长文档的嵌入</li><li>保持长距离依赖关系</li><li>提升长文本检索质量</li></ul><h3 id="_4-实时更新与增量学习" tabindex="-1">4. 实时更新与增量学习 <a class="header-anchor" href="#_4-实时更新与增量学习" aria-label="Permalink to &quot;4. 实时更新与增量学习&quot;">​</a></h3><ul><li>支持知识实时更新</li><li>增量学习新知识</li><li>减少重新训练成本</li></ul><h3 id="_5-可解释性增强" tabindex="-1">5. 可解释性增强 <a class="header-anchor" href="#_5-可解释性增强" aria-label="Permalink to &quot;5. 可解释性增强&quot;">​</a></h3><ul><li>理解向量各维度的含义</li><li>可视化嵌入空间</li><li>提升模型可解释性</li></ul><h2 id="总结" tabindex="-1">总结 <a class="header-anchor" href="#总结" aria-label="Permalink to &quot;总结&quot;">​</a></h2><p>向量嵌入是大语言模型应用中的核心技术，它将离散的文本数据转换为连续的向量表示，使得计算机能够理解和处理语义信息。通过合理选择模型、优化计算策略、结合具体应用场景，向量嵌入能够在 RAG、推荐系统、语义搜索等应用中发挥重要作用。</p><p>随着模型技术的不断发展和优化，向量嵌入的质量和效率将持续提升，为更多创新应用提供强大的技术支撑。</p><hr><blockquote><p>💡 <strong>参考资源</strong>：</p><ul><li><a href="https://huggingface.co/models?pipeline_tag=sentence-similarity" target="_blank" rel="noreferrer">Hugging Face Embeddings</a></li><li><a href="https://platform.openai.com/docs/guides/embeddings" target="_blank" rel="noreferrer">OpenAI Embeddings Guide</a></li><li><a href="https://www.sbert.net/" target="_blank" rel="noreferrer">Sentence Transformers Documentation</a></li></ul></blockquote>`,148)]))}const u=a(t,[["render",e]]);export{g as __pageData,u as default};
